Dear reader, this is a file describing the code used for the recent Phytopthora invasion paper by Bronkhorst et al.

The inputdata source of this paper is imaging data in a .nd2 format, taken using a nikon C2 confocal. 
Analysis was performed using matlab scripts, custom written and uploaded into this project.
The imaging data was structured in folders for each experiment with the label:
DATE_DETAILS_SPECIES_XXX, with XXX being the nth experiment in that series.
Inside of each folder, the raw .nd2 data is saved, as is the imaging data in .tif format (exported using nikon NIS software) in the format:
EXPERIMENTNAME_TX_TYYY_CHANNELNAME, with X being the acquisition series number (1,2,3,4), and YYY being the stacknumber (1st stack taken,
2nd stack etc) and CHANNELNAME the name of the PMT channel (eGFP, RhodamineB, TD etc).

Worked out data from the analysis scripts was saved in thesame folder as the original data folder.
The first code used for this are:
NFM_SigmoidalFit_Test_jb
This code runs the core algorymthn for the sigmoidal fitting routine on a chosen tif file (_RhodamineB.tif files are the correct input
for my datasets), full frame. A coarsening can be achieved by adjusting the scale_factor parameters to larger intergers, enabling
quick analysis of a sample to asses if it is useful.
An option to select which slices to analyse in the z-stack is present in the code, but generally all slices were taken.
The full frame of fiew analysis takes a rather long time (about 3 hours for a 512x512 pixel, 41 slices high z-stack), so an ROI selection
is useful to ensure quick analysis. Each ROI is saved (not automatically, the correct working folder must be selected!) in the original data folder.

To select a smaller ROI for analysis, the following code was used:
NFM_raw_analysis_jbv4_ROI_definer_FromGFP
This code runs a slice slection and XY selection on a chosen .tif file, and creates many of the input parameters in a strcut called param.
Don't forget to manually adjust these settings when analysing a new sample, any new settings like the XY sample size etc must be checked
manually. The code loads in 2 z-stacks, saves the .param containing the coordinates of the Field Of View in the present working directory.
The parameters.mat file containing the .param is used to generate the correct ROI window. The window and position are saved as a crude
image in this code as well, making a quick assesment of the chosen ROI possible.
The follwoing code uses the .param file and raw images to generate folders with the required data for sigmoidal analysis from the present .tif files:
NFM_SigmoidalFit_PrepareData_v2ROI
This codes uses the parameters.mat file containg the .param struct, as generated in ROI_definer to generate ROIs to analyse sigmoidally.
ROIs are much smaller then their parent full FOV, resulting in a much faster analysis. 
All timesteps (so each series/stacknumber) are saved in new unique folders, labbeled in the order of appearance in the dirlist. 
Ensure that the DIR command generates the list in the correct order!!!

After generation of the folders containing each timesteps ROI data, this workflow was used:
ROI_define -> Preparedata -> make a folder in the original data folder called ROIXXX
-> cut the data from the original data folder into the ROIXXX folder, including the parameters.mat and image files generated by the ROI_definer.
This flow ensures that no previous parameters.mat file is present in the original data folder, ensuring that no data mixing happens!!
The workflow can then be followed again to define a seond ROI, a third, etc.

After defining the correct ROIs and saving them in their respective folder, the following code was run:
NFM_SigmoidalFit_FitData_controller_v1
It is impervious that you are in the correct ROIXXX folder directory, and not top level original data folder.
This code is a top level code, calling the function "NFM_SigmoidalFit_FitData_v2_smallerboundaryForDetiltingv2" to be run in each folder.
The code moves into each folder, loads in the prepared workspace.mat file with all the relevant data and runs the work function.
Again, having the correct dir command is vital here to ensure proper indexing. The workflow used in this case was usually:
open ((#coresCPU/2) -1) instances of matlab, save the scripts as this number of different versions by adding _1, _2 etc.
Run in each matlab instance a unique controller script version, changing the iterator i to ensure all versions of the scripts are wokring on different folders.
This is a low level parallelzation that works very efficiently without requiring non-communicating versions of iterable scripts,
which are impossible to run in parralel pool matlab (and avoids requiring this package to run the code).

The workfunction "NFM_SigmoidalFit_FitData_v2_smallerboundaryForDetiltingv2" works like this:
it generates several saving parameters (zpos_FPL_final is the final displacement map) at the start, 
filters the raw images after which a for loop is made:
in the loop, each x,y pixels sigmoidal z-profile decay is analysed and the centre determined, which is taken as the height.
(it now runs about 20-30 pixels/second, for a 500x500 pixel sample = 250.000 traces, takes a while to analyse...)
After the loop, a raw displacement map is the output, which needs to be corrected for tilt (a few microns/200 micron, but still...)
A bounding frame of 5 microns thich on the ROI is selected for tilt correction, after which:
in x and y direction a linear fit is performed to determine the slope on bot sides, and the median slope is selected.
The sample is compensated for the slope with the top left pixel as reference point, using a singular value for the slope.
The use of a singular value is important, we only wish to correct for linear large scale displacement and not induce curvature, 
while local curvature (induced by organisms!) are the main interest of the paper. 
After curvature correction the sample is lowered to median height of the sample, to ensure all displacements are from a reference frame of 0.
Analysis of the workfunction is done, and to visualise several plots of the data are made:
- pre-curvature correction
- post-curvature correction
- post-curvature correction in nm (not used in study)
- Error map to show hotspots of analysis failure (highly autofluorescent dead cells/bubbles in the PDMS)
Data is then saved, and the work function is complete.
IMPORTANT: the work function requires the highly popular cbrewer colourmaps, downloadable from:
https://nl.mathworks.com/matlabcentral/fileexchange/34087-cbrewer-colorbrewer-schemes-for-matlab

After this process, either for ROIs or full frames (if the ROI selected is the whole frame of view) each stack has been analysed
as a separate timestep. Each folder contains the data to be analysed, parameters and output data. This organisation is convenient from
an analytical point of view: all the data for each timestep is grouped together. For making a movie etc of all displacement maps this is
however incredibly inconvenient, for which the "NFM_SigmoidalFit_Data_afteranalysis_Copierv2" code was written:
This code searches in the present working directory (ensure that this is the ROIXXX folder) for folders (where the data is saved),
copies the post-curvature correction image ('z_aftertiltcorr_nonrenorm.tiff') to the saving folder and that is it.
It is IMPORTANT that the dir list to index the folders is in the CORRECT ORDER!
Other versions of this code have been made to copy different types of file, please feel free to use this code structure and to expand!

To copy the zpos_final output data from each timestep, a version of the code of the paragraph above was used called:
'Data_Extraction_for_displacements' This is imporant for displacement analysis, to have all the workspace data in 1 folder!

Another code to generate called 'NFM_SigmoidalFit_Data_GFP_moviemaker' was made,
which is employed in the directory of the original data folder. It works much like the "NFM_SigmoidalFit_PrepareData_v2ROI" code 
(and requires a parameters.mat file!), but instead of making a lof of new directories it saves the images directly in 2 versions:
- a full stack average of the ROI
- a selected average (slice input can be governed) of the ROI. Can be useful to filter out top or bottom noise/increase contrast.
The savetarget selects the savind directory, I recommend saving directly into a subfolder of the previously generated ROI Folder
called 'TD' or 'GFP', making it easy to generate a good field of view movie together with the displacement data.

-----------------------------ANALYSIS OF DISPLACEMENT MAPS----------------------------------

After computing all of the displacement maps of to be analysed ROI, analysis can be performed. We have chosen in this study to focus
on the displacement profiles, drawing a line from highest adhesion point to deepest indentation point and expanding it.
After obtaining the profile data, we can fit it in a model for the surface displacement!

For the displacement analysis, first we look into the ROI to asses how to draw a profile over the found displacement map (zpos_final).
This was done using the code: Code_Arb_Profile_Avg_v2_MinMaxROI_V2. This code:
-Indexes all of the worksapces with displacement data
-in each folder, loads in the workspace (zpos_final, if not included, it will load it seperately, check this!)
for the loading of the workspace, all of them must be present in a single folder to be indexed!!!!
-when running for the first time, select the subwindow as described in lines 68!!!! Subwindow ensures high positions from dead
autofluorescent cells that have no physical meaning are not locked on by this code. Good ROI selection at the start helps.
-In the subwindow, the code searches for the highest and lowest points (exect found points to jump around when no indentation is
yet present, upon indentation it tracks really well).
-This vector is expanded with 30 points in both directions to generate startpoints and endpoints for the analysis
-A number of points to the side is declared (10 is standard, but please adjust to the sample size)
-For each point a line is drawn, and a diagram of this is plotted (Z_profile_TXXXnormal.png) 
+ zoom-in image of ROI (Z_profile_TXXXzoom.png)
-Extract the profile data taken
-save plots of the data etc.
Due to the difference of plotting maps using either imagesc vs implot/scatter, the ydirection of images can become flipped. 
Take this into account when making movies later on -> I personally flipped the direction back in ImageJ (Fiji) to compensate.
It is important to update select_ROI_position_round=[x,y,width,height] (width, height are perhaps flipped) to the ROI sto be analysed!
Again, this code requires cbrewer colourmaps.

The work function for the fitting is Profiles_2regions_v2_Revision2:
this functions accepts the displacement profile observations and input parameters, and return a fit from those parameters.
It can give an error if extreme values are inputted in the parameters.
Within this function is an option to compute stresses as well as a reconstrcution of the ellipsoidal fits, which is generally shut off
because it is not needed to asses the goodness of fit. DONT FORGET to turn the calculate_stresses = True when running the 
SMARTFIT_computestresses_Revision2_UseThisVersion script, otherwise no stress data will be the result of the function. The computation
for stresses can be quite laborius, usually taken about 1 hour for 70 timesteps for the data in this paper.

After obtaining the displacement profile data, the displacement profile is fitted using the code:
'smartfit_try_catch_withreverse_v3_allfitparam_at_once_UpThresh'
This code does the following steps:
-Load in the profile data as generated by the Code_Arb_Profile_Avg_v2_MinMaxROI_V2, labelled 'Z_Profile_Arb_profile_data_TXXX.mat'
-declare some starting variables (number iterations, start guesses for parameters etc)
-compute the first SumOfSquares (the error of fit), which is to be minimised
-iteraively change all parameters using a bandwith (and some maxima to ensure noise fitting the first timesteps goes as well as possible)
-asses if the new set of parameters gives a better fit, if so, accept, if not, reject but with a chance of acceptance
to ensure local minima can be left.
-after all interations, the complete parameter sets tried and SoS values are saved, and the final fit is plotted and saved.
Again, this code can be parralised like the NFM_SigmoidalFit_FitData_controller_v1 script.

After obtaining the displacement profile data, the displacement profile is fitted using the code:
SMARTFIT_computestresses_Revision2_UseThisVersion
The work function in the script (Profiles_2regions_v2_Revision2, dont forget to switch on the calculate_stresses by setting it to True),
will plug out the stress data and save it, make plot of all 3 stress components as a funtion of the profile and compoute from the
found parameters the indentive force via a Hertz equation. Plots of stresses etc are also made and saved.

-----------------------------ANALYSIS OF GFP MAPS (TubeAnalyser)----------------------------------

Next to profile analysis of the GFP map, similar profile analysis can be performed starting from the GFP data. 
To select a spot for analysis, first a code called 'Intensity_profile' is run. 
This code is much like the Code_Arb_Profile_Avg_v2_MinMaxROI_V2, but does not automatically search for a image and gets the positions.
Instead, this version allows manual selection of the file to be analysed, and manual placement of the start and endpoint of the profile.
In our studies usually placed the start behind the cyst, and the stop past the apex of the germ tube tip in the last timepoint measured.
Similar to ROI_definer, the most important data from this operation are the positions_line and n_pixels_side parameters, describing
both the position of the to be analysed region and the amount of lines to take to the side (width of the data later). The stardardisation
of anlaysis direction plus this script ensure that the dataset is correctly oriented when analysing in later scripts:
cyst to the left, germ tube germinating to the right direction and everything is in the frame.

After selection of the correct spot, this part of the Intensity_profile was commented out and the found parameters were directly
used as input without asking the user at each iteration. To buid up all the data for analysis, the code was run manual while selecting
the first stack to the last (this is quite laborius!). Although we considered automating this, we never did it beause the students
working with the data did not mind doing these steps. I do wish to encourage others in the future to do so, all elements needed can be
found in the codes above (index on the _GFP images, run loop over each image and correct site, save data for each timestep)

To save the correct positions (expand the list for multiple samples!), we chose to save this data in a text file in the following format:
(as taken from 200122linepos.txt files)
[name] [no_pixels_side] [positions_line]
ROI 1: pixel size: 20 [3.196152073732719e+02,95.287172011661820;3.727027649769585e+02,1.893279883381924e+02]

After converting all raw GFP images to a analysed region and saving, the code 'From_ArbProfileData_GermTubeSelect_Analysis_Step2' 
was run. This code:
-indexes all arb_profile....mat data
-finds the last sample (germ tube final position)
-manual selection of site of background (rectangle) to be analysed in each frame
-then we start the loop:
-load in the raw GFP intensity map
-cut out the selected site for background anlaysis
-fit lognormal to intensity distribution of background
-from fit, retrieve sigma (average) and mu (st. devation)
-generate binarization criterion (exp(mu+sigma_number*sigma)), usual sigma_number is 3.
-create a binarised mask (used later as input for TubeAnalyser)
-save data and repeat loop until done. (the weighed_thickness.mat file contains all of the intensity and binarised maps)

When all of the binarised maps have been made, we have automatically edge detected the cell position. 
This is used as inputdata for the From_ArbProfileData_GermTubeSelect_Analysis_Step3_TubeAnalyser:
-index all of the weighied_thickness datafiles to loop over them
-before the loop starts, we declare the start positon of the germ tube in the x direction, and the minimum tube length before we
consider it before analysis.
-then we loop over the data:
-load in the intensity/binarised maps
-using bwmorph, smooth and thin the germ tube to a centre line (euler conserved, so no breaks in the line!)
-multiply the binarised map and the intensity map -> all intensities not on a binerised site will not be used for averageing of
germ tube intensity, effectively filtering out background (although this signal is much weaker then the GFP, usually by a factor of 10)
-In the new intensity map, check if the length of the found centre line is longer then the minimum distance
-if so, we follow the line from left (smallest x) to right, gnerating coordinates of this site and the x+distance site for local
intensity analysis.
-local intensity analysis is done by the work function Code_Arb_Profile_Avg_complete_function, returning an intensity for this small
segment, which is averaged to 1 intensity for this x position.
-then we loop to the next position, until we have reached the last_x-distance, after which we cannot know the coordinates.
-The data obtained after the loop is an intensity profile of the tube, localised on the tube.
-This data we then analyse for several parameters and plot
-the important parameter is alpha, which is defined as: 
mean(intensity of points starting from minimum tube length)/max intensity found in the tube)-1.
We found alpha to be a good measure for the polarity in the tube; no polarity=0, high polarity was usually around 1.
-The code finishes with creating quite a few plots and saving data, the plots being the confirmation that the correct sites etc were analysed.
the start positon of the germ tube can be tricky, it will take a few tries usually to get this right. I personalyl try the last timestep
to asses where I would place this point, and then asses if there is major cyst movement after germination of the germ tube. If so,
finding a balance can be tricky, although after the start of invasion the cyst movement is minimal.

The work function Code_Arb_Profile_Avg_complete_function is much alike the Code_Arb_Profile codes, as discussed above.



